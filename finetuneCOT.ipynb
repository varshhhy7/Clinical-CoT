{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "481a63ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m154.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.2/295.2 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m130.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
      "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m  GPU: Tesla T4\n",
      "  VRAM: 15.8 GB\n",
      "  CUDA: 12.8\n",
      "  Available VRAM: 15.7 GB\n",
      "PyTorch: 2.9.1+cu128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \\\n",
    "    \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" \\\n",
    "    datasets \\\n",
    "    evaluate \\\n",
    "    rouge-score \\\n",
    "    peft \\\n",
    "    accelerate \\\n",
    "    bitsandbytes \\\n",
    "    transformers \\\n",
    "    trl \\\n",
    "    gradio \\\n",
    "    xformers \\\n",
    "    scikit-learn\n",
    "\n",
    "import os\n",
    "import platform\n",
    "import torch\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"  GPU: {gpu_name}\")\n",
    "    print(f\"  VRAM: {gpu_memory_gb:.1f} GB\")\n",
    "    print(f\"  CUDA: {torch.version.cuda}\")\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\n",
    "    torch.cuda.memory.set_per_process_memory_fraction(0.95)\n",
    "    \n",
    "    print(f\"  Available VRAM: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\" No GPU detected - check Runtime > Change runtime type\")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0932f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79b3abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN set: True\n",
      "WANDB_API_KEY set: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanivarshithpc\u001b[0m (\u001b[33mmanivarshithpc-vignan-institute-of-technology-and-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "wandb_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "print(\"HF_TOKEN set:\", bool(hf_token))\n",
    "print(\"WANDB_API_KEY set:\", bool(wandb_key))\n",
    "\n",
    "if hf_token:\n",
    "    from huggingface_hub import login\n",
    "    login(token=hf_token)\n",
    "\n",
    "if wandb_key:\n",
    "    import wandb\n",
    "    wandb.login(key=wandb_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f6cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "print(f\"✓ Seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43eab894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Model: unsloth/DeepSeek-R1-Distill-Llama-8B-bnb-4bit\n",
      "HF Repo: varshith7/deepseek-medical-cot\n",
      "W&B Project: deepseek-medical-cot\n",
      "Effective Batch Size: 8\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME_BASE = \"unsloth/DeepSeek-R1-Distill-Llama-8B-bnb-4bit\"  \n",
    "\n",
    "HF_REPO_ID = os.getenv(\"HF_REPO_ID\", \"varshith7/deepseek-medical-cot\")\n",
    "WANDB_PROJECT = os.getenv(\"WANDB_PROJECT\", \"deepseek-medical-cot\")\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    # Training schedule\n",
    "    \"num_train_epochs\": 3,  \n",
    "    \"train_batch_size\": 1,  \n",
    "    \"gradient_accumulation_steps\": 8,  \n",
    "    \n",
    "    \n",
    "    \"learning_rate\": 2e-5,  \n",
    "    \"lr_scheduler_type\": \"cosine\", \n",
    "    \"warmup_ratio\": 0.1,  \n",
    "    \n",
    "    # Sequence and optimization\n",
    "    \"max_length\": 1024, \n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_grad_norm\": 0.3,  \n",
    "    \n",
    "    # Logging and evaluation\n",
    "    \"logging_steps\": 5,\n",
    "    \"eval_steps\": 50,  \n",
    "    \"save_steps\": 100,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"save_total_limit\": 2,\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {MODEL_NAME_BASE}\")\n",
    "print(f\"HF Repo: {HF_REPO_ID}\")\n",
    "print(f\"W&B Project: {WANDB_PROJECT}\")\n",
    "print(f\"Effective Batch Size: {hyperparameters['train_batch_size'] * hyperparameters['gradient_accumulation_steps']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5807019f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PubMedQA dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc96bb0814034031aee644975c78b19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afe6a6f19a649729c0f463ca57ee62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pqa_labeled/train-00000-of-00001.parquet:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5aa7abf368490887b52666576053d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38be059cb14e42ed84c8693e72094672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c6107d620143408fbe0c1fcf34af72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Train: 800 | Validation: 200\n",
      "\n",
      "Sample formatted text:\n",
      "### Instruction:\n",
      "You are an expert medical AI assistant. Analyze the clinical information carefully, apply evidence-based reasoning, and provide a clear step-by-step explanation before giving your final answer.\n",
      "\n",
      "### Input:\n",
      "Context: contexts: ['Figures from the British Defence Dental Services reveal that serving personnel in the British Army have a persistently lower level of dental fitness than those in the Royal Navy or the Royal Air Force. No research had been undertaken to ascertain if this r...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "print(\"Loading PubMedQA dataset...\")\n",
    "raw_dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "\n",
    "# Create larger validation set for better evaluation\n",
    "split_dataset = raw_dataset[\"train\"].train_test_split(test_size=200, seed=SEED)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# Enhanced system prompt for medical reasoning\n",
    "system_prompt = (\n",
    "    \"You are an expert medical AI assistant. Analyze the clinical information carefully, \"\n",
    "    \"apply evidence-based reasoning, and provide a clear step-by-step explanation before \"\n",
    "    \"giving your final answer.\"\n",
    ")\n",
    "\n",
    "def format_sample_enhanced(example):\n",
    "    \"\"\"Enhanced formatting with better structure\"\"\"\n",
    "    context = example.get(\"context\", {})\n",
    "    question = example.get(\"question\", \"\")\n",
    "    long_answer = example.get(\"long_answer\", \"\")\n",
    "    \n",
    "    # Join context if it's a dict with multiple sections\n",
    "    if isinstance(context, dict):\n",
    "        context_text = \" \".join([f\"{k}: {v}\" for k, v in context.items()])\n",
    "    else:\n",
    "        context_text = str(context)\n",
    "    \n",
    "    # DeepSeek-R1 style formatting with chain-of-thought\n",
    "    formatted_text = f\"\"\"### Instruction:\n",
    "{system_prompt}\n",
    "\n",
    "### Input:\n",
    "Context: {context_text}\n",
    "Question: {question}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "Let me analyze this medical question step by step:\n",
    "1. Understanding the context and key information\n",
    "2. Applying clinical reasoning\n",
    "3. Formulating the answer based on evidence\n",
    "</think>\n",
    "\n",
    "<answer>\n",
    "{long_answer}\n",
    "</answer>\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"text\": formatted_text,\n",
    "        \"question\": question,\n",
    "        \"answer\": long_answer\n",
    "    }\n",
    "\n",
    "print(\"Formatting dataset...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    format_sample_enhanced, \n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=2  # Parallel processing\n",
    ")\n",
    "val_dataset = val_dataset.map(\n",
    "    format_sample_enhanced, \n",
    "    remove_columns=val_dataset.column_names,\n",
    "    num_proc=2\n",
    ")\n",
    "\n",
    "print(f\"✓ Train: {len(train_dataset)} | Validation: {len(val_dataset)}\")\n",
    "print(\"\\nSample formatted text:\")\n",
    "print(train_dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45aa2a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer from unsloth/DeepSeek-R1-Distill-Llama-8B-bnb-4bit...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d9f6224df24849bf5f80c5d8889d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f83e81e79364aceb83c1de2331de0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a53312b258c4c4594e0f039ffb884b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48e569677a1434d9c751e69bedaca90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizer loaded\n",
      "  Vocab size: 128256\n",
      "  PAD token: <|finetune_right_pad_id|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(f\"\\nLoading tokenizer from {MODEL_NAME_BASE}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_BASE, use_fast=True)\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"  Tokenizer loaded\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff81c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
